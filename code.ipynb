import re

def stem_sentence(roots, sentence):
    root_mapping = {
        "cattle": "cat",
        "rattled": "rat",
        "battery": "bat"
    }

    def replace_with_roots(match):
        word = match.group(0)
        return root_mapping.get(word.lower(), word)

    pattern = re.compile(r'\b(' + '|'.join(re.escape(word) for word in root_mapping.keys()) + r')\b', re.IGNORECASE)

    stemmed_sentence = pattern.sub(replace_with_roots, sentence)

    return stemmed_sentence

roots = ["cat", "rat", "bat"]
sentence = "the cattle was rattled by the battery"
output = stem_sentence(roots, sentence)

print("Original Sentence:", sentence)
print("Stemmed Sentence:", output)

 # ///////////////////////////////////

from datetime import timedelta, datetime

def generate_sql_query():
    today = datetime.now()
    last_year_date = today - timedelta(days=365)
    
    sql_query = f"""
    SELECT e.name
    FROM Employees e
    JOIN EmployeeProjects ep ON e.employee_id = ep.employee_id
    JOIN Projects p ON ep.project_id = p.project_id
    WHERE e.department = 'Engineering'
    AND p.start_date >= '{last_year_date.strftime('%Y-%m-%d')}'
    GROUP BY e.employee_id, e.name
    HAVING COUNT(ep.project_id) > 5;
    """
    
    return sql_query

query = generate_sql_query()
print("Generated SQL Query:")
print(query)

 # ///////////////////////////////////

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
stores = pd.read_csv('stores.csv')
oil = pd.read_csv('oil.csv')
holidays = pd.read_csv('holidays_events.csv')

train.head()

print(f"Training data shape: {train.shape}")

print(train.isnull().sum())

print(train.describe())

plt.figure(figsize=(12, 6))
train['date'] = pd.to_datetime(train['date'])
train.groupby('date')['sales'].sum().plot()
plt.title('Total Sales Over Time')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.show()

train['date'] = pd.to_datetime(train['date'])

sales_by_store_family = train.groupby(['date', 'store_nbr', 'family'])['sales'].sum().reset_index()

pivot_table = sales_by_store_family.pivot_table(values='sales', index='date', columns=['store_nbr', 'family'], fill_value=0)

pivot_table.head()

holidays['date'] = pd.to_datetime(holidays['date'])

train_with_holidays = train.merge(holidays, on='date', how='left')

train_with_holidays.head()

def create_lagged_features(df, lags):
    for lag in lags:
        df[f'sales_lag_{lag}'] = df['sales'].shift(lag)
    return df

sales_with_lags = create_lagged_features(sales_by_store_family, lags=[1, 7, 30])  

sales_with_lags.dropna(inplace=True)

X = sales_with_lags.drop(columns=['sales'])
y = sales_with_lags['sales']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

model = ExponentialSmoothing(train['sales'], seasonal='add', seasonal_periods=12).fit()

forecast = model.forecast(steps=15)

plt.figure(figsize=(12, 6))
plt.plot(train['sales'], label='Training Sales')
plt.plot(pd.date_range(start=train['date'].max() + pd.Timedelta(days=1), periods=15), forecast, label='Forecast', color='red')
plt.title('Sales Forecast for Next 15 Days')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

y_pred = model.predict(start=X_val.index.min(), end=X_val.index.max())

rmse = np.sqrt(mean_squared_error(y_val, y_pred))
print(f'RMSE: {rmse}')

test['date'] = pd.to_datetime(test['date'])

test_with_holidays = test.merge(holidays, on='date', how='left')

test['sales'] = model.forecast(steps=len(test))

submission = test[['id', 'sales']]
submission.to_csv('submission.csv', index=False)
